# Model Configuration for Story Theory Benchmark
# Specifies model selection criteria and generation/evaluation parameters

model_tiers:
  strong:
    description: "High-capability models for establishing performance ceiling"
    price_range: [8, 15]  # $/1M output tokens
    min_context: 100000
    capabilities:
      - long_context
      - instruction_following
    expected_pass_rate: [0.70, 0.80]
    # Example models: Claude Sonnet 4, GPT-4o, Gemini 1.5 Pro

  cheap:
    description: "Cost-effective models for baseline comparison"
    price_range: [0.20, 0.80]  # $/1M output tokens
    min_context: 32000
    capabilities:
      - instruction_following
    expected_pass_rate: [0.45, 0.60]
    # Example models: Claude Haiku, Gemini Flash, GPT-4o-mini

# Model selection query for OpenRouter
openrouter:
  base_url: "https://openrouter.ai/api/v1"
  models_endpoint: "/models"

generation:
  temperature: 0.7
  max_tokens: 2000
  samples_per_task: 3
  retry_attempts: 3
  retry_delay: 5  # seconds

evaluation:
  temperature: 0.1  # Low for consistency
  max_tokens: 1000
  response_format: "json_object"
  # Use cheap tier model for evaluation
  evaluator_tier: "cheap"

# Benchmark generation models (OpenRouter model IDs)
benchmark_models:
  # Anthropic
  - anthropic/claude-opus-4.5
  - anthropic/claude-sonnet-4.5
  - anthropic/claude-sonnet-4
  - anthropic/claude-haiku-4.5
  # OpenAI
  - openai/o3
  - openai/o3-mini
  - openai/gpt-5.2
  - openai/gpt-5.1
  - openai/gpt-5
  - openai/gpt-4o
  - openai/gpt-4o-mini
  # Google
  - google/gemini-3-pro-preview
  - google/gemini-2.5-flash
  # DeepSeek
  - deepseek/deepseek-r1
  - deepseek/deepseek-v3.2
  # Others
  - x-ai/grok-4
  - qwen/qwen3-235b-a22b
  - meta-llama/llama-4-maverick
  - mistralai/ministral-14b-2512
  - minimax/minimax-m2
